{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build to Manage - Node.js application monitoring and logging lab During this lab we will instrument a sample Node.js application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the application code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release. Lab outline Fork and clone the Github repository and review the source code Configure logging library and add log messages Configure and run Elastic stack in Docker Compose Integrate application logs with Elastic stack What to instrument? - the RED method Instrument application code with Node.js client library for Prometheus Enable a default set of metrics offered by prom-client library Define custom metrics (counter and histogram) Configure and run Prometheus and Grafana stack in Docker Compose Run example PromQL queries Configure sample alert in Prometheus Configure Grafana Import sample dashboard Define a custom dashboard panel Deploy to IBM Cloud Private cluster and configure monitoring using ICP Prometheus and Grafana Configure kubernetes liveness probe using application health check Prerequisites Install the following software on your workstation or use the provided VM. Node.js npm Docker Docker Compose kubectl Review the application code and run it locally Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/b2m-nodejs From now on you will work on your own fork of the application project https://github.com/ username /b2m-nodejs where username is your GitHub username. Clone the b2m-nodejs lab repository to your home directory using: cd git clone https://github.com/ username /b2m-nodejs Most of the commands in this lab should be executed from the b2m-nodejs/src directory: cd b2m-nodejs/src Review the application code in server.js . Application simulates a transaction with random response time on the following URL: http://localhost:3001/checkout About 20% of requests should return error and 500 HTTP response code. Run the Node Package Manager to install all required node modules: npm install and then npm start server.js to start the application. Node.js server should start and listen on port 3001 . Use internet browser to access http://localhost:3001/checkout { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected } Refresh the page a couple of times and you should see random transaction response times.","title":"1. Introduction"},{"location":"#build-to-manage-nodejs-application-monitoring-and-logging-lab","text":"During this lab we will instrument a sample Node.js application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the application code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release.","title":"Build to Manage - Node.js application monitoring and logging lab"},{"location":"#lab-outline","text":"Fork and clone the Github repository and review the source code Configure logging library and add log messages Configure and run Elastic stack in Docker Compose Integrate application logs with Elastic stack What to instrument? - the RED method Instrument application code with Node.js client library for Prometheus Enable a default set of metrics offered by prom-client library Define custom metrics (counter and histogram) Configure and run Prometheus and Grafana stack in Docker Compose Run example PromQL queries Configure sample alert in Prometheus Configure Grafana Import sample dashboard Define a custom dashboard panel Deploy to IBM Cloud Private cluster and configure monitoring using ICP Prometheus and Grafana Configure kubernetes liveness probe using application health check","title":"Lab outline"},{"location":"#prerequisites","text":"Install the following software on your workstation or use the provided VM. Node.js npm Docker Docker Compose kubectl","title":"Prerequisites"},{"location":"#review-the-application-code-and-run-it-locally","text":"Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/b2m-nodejs From now on you will work on your own fork of the application project https://github.com/ username /b2m-nodejs where username is your GitHub username. Clone the b2m-nodejs lab repository to your home directory using: cd git clone https://github.com/ username /b2m-nodejs Most of the commands in this lab should be executed from the b2m-nodejs/src directory: cd b2m-nodejs/src Review the application code in server.js . Application simulates a transaction with random response time on the following URL: http://localhost:3001/checkout About 20% of requests should return error and 500 HTTP response code. Run the Node Package Manager to install all required node modules: npm install and then npm start server.js to start the application. Node.js server should start and listen on port 3001 . Use internet browser to access http://localhost:3001/checkout { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected } Refresh the page a couple of times and you should see random transaction response times.","title":"Review the application code and run it locally"},{"location":"ICP/","text":"Containerize the app and deploy to IBM Cloud Private Create a Docker container Use provided Dockerfile to build the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs . Test container locally (make sure you stopped the local Node.js server and removed previous version of this docker container). docker stop b2m-nodejs docker rm b2m-nodejs docker run --name btm-nodejs -d -p 3001:3001 b2m-nodejs Access http://localhost:3001 to verify the application is running. Now, our Node.js application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-nodejs application container image has been uploaded to public Docker Hub: rszypulka/b2m-nodejs . You can also upload it to the local ICP Container Registry. Deploy b2m-nodejs application to the ICP cluster Review provided YAML file b2m-nodejs-icp.yml and use it to deploy the application to IBM Cloud Private cluster. b2m-nodejs deployment object will pull application image container rszypulka/b2m-nodejs from Docker Hub. kubectl apply -f b2m-nodejs-icp.yml Verify deployment status. $ kubectl get deploy b2m-nodejs -n default NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE b2m-nodejs 1 1 1 1 4h Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default -o jsonpath= {.spec.ports[0].nodePort} services b2m-nodejs) export NODE_IP=$(kubectl get nodes -l proxy=true -o jsonpath= {.items[0].status.addresses[?(@.type==\\ Hostname\\ )].address} ) echo http://$NODE_IP:$NODE_PORT Use the browser to access the application URL: http:// node_external_ip : external_nodeport Enable monitoring using ICP Prometheus and Grafana Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-nodejs scrape_interval: 20s static_configs: - targets: - b2m-nodejs.default.svc:80 labels: service: my-service group: production Make sure the indentation is correct. Import provided Grafana dashboard grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics. Define the kubernetes liveness probe for use with built-in application health check The provided b2m-nodejs-icp.yml deployment YAML file define liveness probe that uses the implemented /healthz route. livenessProbe: httpGet: path: /healthz port: 3001 initialDelaySeconds: 3 periodSeconds: 10 Check the URL: http:// node_external_ip : external_nodeport /healthz to verify current health status. Expected output: {\"status\":\"ok\"} Simulate an application problem by setting \"bad health\" status: http:// node_external_ip : external_nodeport /bad-health Expected output: {\"status\":\"App health set to 'false'\"} Quickly check the health URL again: http:// node_external_ip : external_nodeport /healthz Expected output: {\"error\":\"Application unhealthy\"} After up to 10s the application pod should be automatically restarted by kubernetes and health set back to {\"status\":\"ok\"} Expected output: {\"status\":\"ok\"} Verify the pod has been restarted. kubectl describe pod b2m-nodejs in the Events section you should see events similar to the following: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 3m (x9 over 58m) kubelet, 172.16.254.163 Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Pulling 2m (x4 over 1h) kubelet, 172.16.254.163 pulling image rszypulka/b2m-nodejs Normal Killing 2m (x3 over 58m) kubelet, 172.16.254.163 Killing container with id docker://node-prom:Container failed liveness probe.. Container will be killed and recreated. Normal Pulled 2m (x4 over 1h) kubelet, 172.16.254.163 Successfully pulled image rszypulka/b2m-nodejs Normal Created 2m (x4 over 1h) kubelet, 172.16.254.163 Created container Normal Started 2m (x4 over 1h) kubelet, 172.16.254.163 Started container","title":"5. Deploy to IBM Cloud Private"},{"location":"ICP/#containerize-the-app-and-deploy-to-ibm-cloud-private","text":"","title":"Containerize the app and deploy to IBM Cloud Private"},{"location":"ICP/#create-a-docker-container","text":"Use provided Dockerfile to build the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs . Test container locally (make sure you stopped the local Node.js server and removed previous version of this docker container). docker stop b2m-nodejs docker rm b2m-nodejs docker run --name btm-nodejs -d -p 3001:3001 b2m-nodejs Access http://localhost:3001 to verify the application is running. Now, our Node.js application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-nodejs application container image has been uploaded to public Docker Hub: rszypulka/b2m-nodejs . You can also upload it to the local ICP Container Registry.","title":"Create a Docker container"},{"location":"ICP/#deploy-b2m-nodejs-application-to-the-icp-cluster","text":"Review provided YAML file b2m-nodejs-icp.yml and use it to deploy the application to IBM Cloud Private cluster. b2m-nodejs deployment object will pull application image container rszypulka/b2m-nodejs from Docker Hub. kubectl apply -f b2m-nodejs-icp.yml Verify deployment status. $ kubectl get deploy b2m-nodejs -n default NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE b2m-nodejs 1 1 1 1 4h Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default -o jsonpath= {.spec.ports[0].nodePort} services b2m-nodejs) export NODE_IP=$(kubectl get nodes -l proxy=true -o jsonpath= {.items[0].status.addresses[?(@.type==\\ Hostname\\ )].address} ) echo http://$NODE_IP:$NODE_PORT Use the browser to access the application URL: http:// node_external_ip : external_nodeport","title":"Deploy b2m-nodejs application to the ICP cluster"},{"location":"ICP/#enable-monitoring-using-icp-prometheus-and-grafana","text":"Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-nodejs scrape_interval: 20s static_configs: - targets: - b2m-nodejs.default.svc:80 labels: service: my-service group: production Make sure the indentation is correct. Import provided Grafana dashboard grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics.","title":"Enable monitoring using ICP Prometheus and Grafana"},{"location":"ICP/#define-the-kubernetes-liveness-probe-for-use-with-built-in-application-health-check","text":"The provided b2m-nodejs-icp.yml deployment YAML file define liveness probe that uses the implemented /healthz route. livenessProbe: httpGet: path: /healthz port: 3001 initialDelaySeconds: 3 periodSeconds: 10 Check the URL: http:// node_external_ip : external_nodeport /healthz to verify current health status. Expected output: {\"status\":\"ok\"} Simulate an application problem by setting \"bad health\" status: http:// node_external_ip : external_nodeport /bad-health Expected output: {\"status\":\"App health set to 'false'\"} Quickly check the health URL again: http:// node_external_ip : external_nodeport /healthz Expected output: {\"error\":\"Application unhealthy\"} After up to 10s the application pod should be automatically restarted by kubernetes and health set back to {\"status\":\"ok\"} Expected output: {\"status\":\"ok\"} Verify the pod has been restarted. kubectl describe pod b2m-nodejs in the Events section you should see events similar to the following: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 3m (x9 over 58m) kubelet, 172.16.254.163 Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Pulling 2m (x4 over 1h) kubelet, 172.16.254.163 pulling image rszypulka/b2m-nodejs Normal Killing 2m (x3 over 58m) kubelet, 172.16.254.163 Killing container with id docker://node-prom:Container failed liveness probe.. Container will be killed and recreated. Normal Pulled 2m (x4 over 1h) kubelet, 172.16.254.163 Successfully pulled image rszypulka/b2m-nodejs Normal Created 2m (x4 over 1h) kubelet, 172.16.254.163 Created container Normal Started 2m (x4 over 1h) kubelet, 172.16.254.163 Started container","title":"Define the kubernetes liveness probe for use with built-in application health check"},{"location":"Prometheus-Grafana/","text":"Deploy a local Prometheus and Grafana stack with Docker Compose During this lab we will run the Prometheus and Grafana in Docker Compose. Configuration for this lab is based on https://github.com/vegasbrianc/prometheus . In the lab VM the Prometheus docker compose project was cloned to /root/prometheus . 1). Add the scraping job definition to the Prometheus configuration file /root/prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-nodejs scrape_interval: 20s static_configs: - targets: [ xxx.xxx.xxx.xxx:3001 ] labels: service: btm-nodejs group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. In the Skytap lab VM, the IP address should be 10.0.0.1 . 2). Start Prometheus Grafana stack: cd /root/prometheus docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus server was started via: http://localhost:9090 Check the status of scraping targets in Prometheus UI - Status - Targets Run example PromQL queries Generate some application load before running the queries: cd ~/b2m-nodejs/src/ ./load_test.sh Run the following example PromQL queries using the Prometheus UI. Throughput Error rate Range[0,1]: number of 5xx requests / total number of requests sum(increase(http_request_duration_ms_count{code=~ ^5..$ }[1m])) / sum(increase(http_request_duration_ms_count[1m])) Expected value ~0.2 because our application should return 500 for about 20% of transactions. Request Per Minute sum(rate(http_request_duration_ms_count[1m])) by (service, route, method, code) * 60 Check the graph. Response Time Apdex Apdex score approximation: 100ms target and 300ms tolerated response time (sum(rate(http_request_duration_ms_bucket{le= 100 }[1m])) by (service) + sum(rate(http_request_duration_ms_bucket{le= 300 }[1m])) by (service) ) / 2 / sum(rate(http_request_duration_ms_count[1m])) by (service) Note that we divide the sum of both buckets. The reason is that the histogram buckets are cumulative. The le=\"100\" bucket is also contained in the le=\"300\" bucket; dividing it by 2 corrects for that. - Prometheus docs 95th Response Time histogram_quantile(0.95, sum(rate(http_request_duration_ms_bucket[1m])) by (le, service, route, method)) Median Response Time histogram_quantile(0.5, sum(rate(http_request_duration_ms_bucket[1m])) by (le, service, route, method)) Average Response Time avg(rate(http_request_duration_ms_sum[1m]) / rate(http_request_duration_ms_count[1m])) by (service, route, method, code) Memory Usage Average Memory Usage In Megabytes. avg(nodejs_external_memory_bytes / 1024 ) by (service) Configure Prometheus alert Alerting rules allows to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. In this lab we will configure one alerting rule for median response time higher than 100ms. Lab instruction: Add the following alert rule to the alert.rules file. In the lab VM it is located in /root/prometheus/prometheus/alert.rules - alert: APIHighMedianResponseTime expr: histogram_quantile(0.5, sum by(le, service, route, method) (rate(http_request_duration_ms_bucket[1m]))) 30 for: 1m annotations: description: {{ $labels.service }}, {{ $labels.method }} {{ $labels.route }} has a median response time above 100ms (current value: {{ $value }}ms) summary: High median response time on {{ $labels.service }} and {{ $labels.method }} {{ $labels.route }} Restart the Prometheus stack: cd ~/prometheus docker-compose down docker-compose up -d Alerts can be listed via Prometheus UI: http://localhost:9090/alerts States of active alerts: pending : firing : Set the Prometheus datasource in Grafana Logon to Grafana via http://localhost:3000 - user: admin - password: foobar Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with these settings: name: Prometheus type: prometheus url: http://localhost:9090 access: browser Configure dashboard Grafana Dashboard to import : ~/b2m-nodejs/src/btm-nodejs-grafana.json Monitoring dashboard was created according to the RED Method principles: Rate ( Thoughput and Checkouts panels) Errors ( Error rate panel) Duration ( 95th Response Time and Median Response Time panels) Review the configuration of each dashboard panel. Check the annotation settings. Define the Apdex score chart using the following query: (sum(rate(http_request_duration_ms_bucket{le= 100 }[1m])) by (service) + sum(rate(http_request_duration_ms_bucket{le= 300 }[1m])) by (service) ) / 2 / sum(rate(http_request_duration_ms_count[1m])) by (service) You can add it to the existing dashboard: Click on the icon Add panel and select Graph panel type. Click on the panel title and select edit. Select Prometheus datasource in the Metrics tab of the panel query editor Copy PromQL to the free form field Verify the results on the panel preview Explore other Graph panel options Every time you need to generate an application traffic, use provided script ~/b2m-nodejs/src/load_test.sh","title":"4. Prometheus and Grafana configuration"},{"location":"Prometheus-Grafana/#deploy-a-local-prometheus-and-grafana-stack-with-docker-compose","text":"During this lab we will run the Prometheus and Grafana in Docker Compose. Configuration for this lab is based on https://github.com/vegasbrianc/prometheus . In the lab VM the Prometheus docker compose project was cloned to /root/prometheus . 1). Add the scraping job definition to the Prometheus configuration file /root/prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-nodejs scrape_interval: 20s static_configs: - targets: [ xxx.xxx.xxx.xxx:3001 ] labels: service: btm-nodejs group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. In the Skytap lab VM, the IP address should be 10.0.0.1 . 2). Start Prometheus Grafana stack: cd /root/prometheus docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus server was started via: http://localhost:9090 Check the status of scraping targets in Prometheus UI - Status - Targets","title":"Deploy a local Prometheus and Grafana stack with Docker Compose"},{"location":"Prometheus-Grafana/#run-example-promql-queries","text":"Generate some application load before running the queries: cd ~/b2m-nodejs/src/ ./load_test.sh Run the following example PromQL queries using the Prometheus UI.","title":"Run example PromQL queries"},{"location":"Prometheus-Grafana/#throughput","text":"","title":"Throughput"},{"location":"Prometheus-Grafana/#error-rate","text":"Range[0,1]: number of 5xx requests / total number of requests sum(increase(http_request_duration_ms_count{code=~ ^5..$ }[1m])) / sum(increase(http_request_duration_ms_count[1m])) Expected value ~0.2 because our application should return 500 for about 20% of transactions.","title":"Error rate"},{"location":"Prometheus-Grafana/#request-per-minute","text":"sum(rate(http_request_duration_ms_count[1m])) by (service, route, method, code) * 60 Check the graph.","title":"Request Per Minute"},{"location":"Prometheus-Grafana/#response-time","text":"","title":"Response Time"},{"location":"Prometheus-Grafana/#apdex","text":"Apdex score approximation: 100ms target and 300ms tolerated response time (sum(rate(http_request_duration_ms_bucket{le= 100 }[1m])) by (service) + sum(rate(http_request_duration_ms_bucket{le= 300 }[1m])) by (service) ) / 2 / sum(rate(http_request_duration_ms_count[1m])) by (service) Note that we divide the sum of both buckets. The reason is that the histogram buckets are cumulative. The le=\"100\" bucket is also contained in the le=\"300\" bucket; dividing it by 2 corrects for that. - Prometheus docs","title":"Apdex"},{"location":"Prometheus-Grafana/#95th-response-time","text":"histogram_quantile(0.95, sum(rate(http_request_duration_ms_bucket[1m])) by (le, service, route, method))","title":"95th Response Time"},{"location":"Prometheus-Grafana/#median-response-time","text":"histogram_quantile(0.5, sum(rate(http_request_duration_ms_bucket[1m])) by (le, service, route, method))","title":"Median Response Time"},{"location":"Prometheus-Grafana/#average-response-time","text":"avg(rate(http_request_duration_ms_sum[1m]) / rate(http_request_duration_ms_count[1m])) by (service, route, method, code)","title":"Average Response Time"},{"location":"Prometheus-Grafana/#memory-usage","text":"","title":"Memory Usage"},{"location":"Prometheus-Grafana/#average-memory-usage","text":"In Megabytes. avg(nodejs_external_memory_bytes / 1024 ) by (service)","title":"Average Memory Usage"},{"location":"Prometheus-Grafana/#configure-prometheus-alert","text":"Alerting rules allows to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. In this lab we will configure one alerting rule for median response time higher than 100ms. Lab instruction: Add the following alert rule to the alert.rules file. In the lab VM it is located in /root/prometheus/prometheus/alert.rules - alert: APIHighMedianResponseTime expr: histogram_quantile(0.5, sum by(le, service, route, method) (rate(http_request_duration_ms_bucket[1m]))) 30 for: 1m annotations: description: {{ $labels.service }}, {{ $labels.method }} {{ $labels.route }} has a median response time above 100ms (current value: {{ $value }}ms) summary: High median response time on {{ $labels.service }} and {{ $labels.method }} {{ $labels.route }} Restart the Prometheus stack: cd ~/prometheus docker-compose down docker-compose up -d Alerts can be listed via Prometheus UI: http://localhost:9090/alerts States of active alerts: pending : firing :","title":"Configure Prometheus alert"},{"location":"Prometheus-Grafana/#set-the-prometheus-datasource-in-grafana","text":"Logon to Grafana via http://localhost:3000 - user: admin - password: foobar Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with these settings: name: Prometheus type: prometheus url: http://localhost:9090 access: browser","title":"Set the Prometheus datasource in Grafana"},{"location":"Prometheus-Grafana/#configure-dashboard","text":"Grafana Dashboard to import : ~/b2m-nodejs/src/btm-nodejs-grafana.json Monitoring dashboard was created according to the RED Method principles: Rate ( Thoughput and Checkouts panels) Errors ( Error rate panel) Duration ( 95th Response Time and Median Response Time panels) Review the configuration of each dashboard panel. Check the annotation settings. Define the Apdex score chart using the following query: (sum(rate(http_request_duration_ms_bucket{le= 100 }[1m])) by (service) + sum(rate(http_request_duration_ms_bucket{le= 300 }[1m])) by (service) ) / 2 / sum(rate(http_request_duration_ms_count[1m])) by (service) You can add it to the existing dashboard: Click on the icon Add panel and select Graph panel type. Click on the panel title and select edit. Select Prometheus datasource in the Metrics tab of the panel query editor Copy PromQL to the free form field Verify the results on the panel preview Explore other Graph panel options Every time you need to generate an application traffic, use provided script ~/b2m-nodejs/src/load_test.sh","title":"Configure dashboard"},{"location":"about/","text":"Author Rafal Szypulka (rafal.szypulka@pl.ibm.com) Acknowledgements https://github.com/RisingStack/example-prometheus-nodejs","title":"About"},{"location":"about/#author","text":"Rafal Szypulka (rafal.szypulka@pl.ibm.com)","title":"Author"},{"location":"about/#acknowledgements","text":"https://github.com/RisingStack/example-prometheus-nodejs","title":"Acknowledgements"},{"location":"logging/","text":"A production service should have both logging and monitoring. Monitoring provides a real-time and historical view on the system and application state, and alerts you in case a situation is met. In most cases, a monitoring alert is simply a trigger for you to start an investigation. Monitoring shows the symptoms of problems. Logs provide details and state on individual transactions, so you can fully understand the cause of problems. Logs provide visibility into the behavior of a running app, they are one of the most fundamental tools for debugging and finding issues within your application. If structured correctly, logs can contain a wealth of information about a specific event. Logs can tell us not only when the event took place, but also provide us with details as to the root cause. Therefore, it is important that the log entries are readable to humans and machines. According to the 12-factor application guidelines, logs are the stream of aggregated, time-ordered events. A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage log files. Instead, each running process writes its event stream, unbuffered, to stdout. If you deviate from these guidelines, make sure that you address the operational needs for log files, such as logging to local files and applying log rotation policies. Configure the logging library Go to the directory where the server.js file is located and run the following command to install and configure a winston logging library for node.js. cd ~/b2m-nodejs/src npm install --save winston this should add the following dependency to the package.json : winston : ^3.2.1 Example implementation of logging Add/uncomment the following line at the beginning of server.js to load the winston module: const { createLogger , format , transports } = require ( winston ) then create the logger object: const logger = createLogger ({ level : debug , format : format . combine ( format . timestamp ({ format : YYYY-MM-DD T HH:mm:ss.SSSZ }), format . json () ), transports : [ new transports . Console ()] }); The configuration above specifies timestamp field format and enables sending logs in json format to STDOUT. Timestamp should include the time zone information and be precise down to milliseconds. Whenever you want to generate a log entry, just use the logger object with level specified methods: error , warn , info , verbose , debug msg = RSAP0010E: Severe problem detected logger . error ( msg ) msg = RSAP0001I: Transaction OK logger . info ( msg ) You can add also additional metadata like errorCode or transactionTime that can be useful in log analytics. Extend your logger statement like on the example below. Additional metadata will be used later in our log analytics dashboard. Look for commented lines starting with logger and uncomment it. msg = RSAP0001I: Transaction OK logger . info ( msg , { errCode : RSAP0001I , transactionTime : delay }) msg = RSAP0010E: Severe problem detected logger . error ( msg , { errorCode : RSAP0010E , transactionTime : delay }) Restart the application ( ctrl-c in the terminal window and start with npm start server.js after every code change). Example STDOUT (in the application terminal window): { errCode : RSAP0001I , transactionTime : 81 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:49.625Z } { errCode : RSAP0010E , transactionTime : 76 , level : error , message : RSAP0010E: Severe problem detected , timestamp : 2019-02-27T07:34:50.008Z } { errCode : RSAP0001I , transactionTime : 22 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.325Z } { errCode : RSAP0001I , transactionTime : 1 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.620Z } { errCode : RSAP0001I , transactionTime : 96 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.871Z } { errCode : RSAP0001I , transactionTime : 62 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:51.156Z } After testing of the logging features, stop the application. Configure your Github user and commit your changes to your GitHub repository: cd ~/b2m-nodejs git config --global user.email your_github_email git commit -am I added logging to my app! git push Access your Github via web browser and verify that you see recent updates and history of changes. Create a Docker image for node.js application Use provided Dockerfile to build the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs . Integrate with the Elastic stack The following procedure shows how to send the application logs to the local Elastic stack running in Docker. Deploy a local Elastic stack with Docker Compose During this lab we will run the Elastic Stack (Elasticsearch, Logstash, Kibana) in the Docker Compose. Configuration for this lab is based on https://github.com/deviantony/docker-elk . In the lab VM the Elastic stack docker compose project was cloned to /root/docker-elk . Briefly review the simple logstash configuration we use for this lab: /root/docker-elk/logstash/pipeline/logstash.conf : input { gelf { port = 5000 } } filter { json { source = message } #we need level field in a numeric format mutate { gsub = [ level , info , 6, level , error , 3 ] } mutate { convert = { level = integer } } } output { elasticsearch { hosts = elasticsearch:9200 } stdout { codec = rubydebug } } The above will reconfigure logstash to use gelf (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using gelf . 1). Start the Elastic stack: cd /root/docker-elk docker-compose up -d Expected output: Creating network docker-elk_elk with driver bridge Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 2). Verify you can access Kibana on http://localhost:5601 . Start the node.js application container and forward logs to Elastic stack Start the application container with this command: docker run --name btm-nodejs -d -p 3001:3001 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-nodejs In the lab environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:3001/checkout : for i in {1..10}; do curl http://localhost:3001/checkout; done and check out the Kibana dashboard: Dashboards- BTM Node.js In case of problems with dashboard, you can also import Kibana configuration using provided btm-nodejs-kibana.json : Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json It should be similar to: Stop and remove the node.js Docker container before starting the next exercise. docker stop btm-nodejs docker rm btm-nodejs","title":"2. Node.js logging and Elastic stack integration"},{"location":"logging/#configure-the-logging-library","text":"Go to the directory where the server.js file is located and run the following command to install and configure a winston logging library for node.js. cd ~/b2m-nodejs/src npm install --save winston this should add the following dependency to the package.json : winston : ^3.2.1","title":"Configure the logging library"},{"location":"logging/#example-implementation-of-logging","text":"Add/uncomment the following line at the beginning of server.js to load the winston module: const { createLogger , format , transports } = require ( winston ) then create the logger object: const logger = createLogger ({ level : debug , format : format . combine ( format . timestamp ({ format : YYYY-MM-DD T HH:mm:ss.SSSZ }), format . json () ), transports : [ new transports . Console ()] }); The configuration above specifies timestamp field format and enables sending logs in json format to STDOUT. Timestamp should include the time zone information and be precise down to milliseconds. Whenever you want to generate a log entry, just use the logger object with level specified methods: error , warn , info , verbose , debug msg = RSAP0010E: Severe problem detected logger . error ( msg ) msg = RSAP0001I: Transaction OK logger . info ( msg ) You can add also additional metadata like errorCode or transactionTime that can be useful in log analytics. Extend your logger statement like on the example below. Additional metadata will be used later in our log analytics dashboard. Look for commented lines starting with logger and uncomment it. msg = RSAP0001I: Transaction OK logger . info ( msg , { errCode : RSAP0001I , transactionTime : delay }) msg = RSAP0010E: Severe problem detected logger . error ( msg , { errorCode : RSAP0010E , transactionTime : delay }) Restart the application ( ctrl-c in the terminal window and start with npm start server.js after every code change). Example STDOUT (in the application terminal window): { errCode : RSAP0001I , transactionTime : 81 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:49.625Z } { errCode : RSAP0010E , transactionTime : 76 , level : error , message : RSAP0010E: Severe problem detected , timestamp : 2019-02-27T07:34:50.008Z } { errCode : RSAP0001I , transactionTime : 22 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.325Z } { errCode : RSAP0001I , transactionTime : 1 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.620Z } { errCode : RSAP0001I , transactionTime : 96 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:50.871Z } { errCode : RSAP0001I , transactionTime : 62 , level : info , message : RSAP0001I: Transaction OK , timestamp : 2019-02-27T07:34:51.156Z } After testing of the logging features, stop the application. Configure your Github user and commit your changes to your GitHub repository: cd ~/b2m-nodejs git config --global user.email your_github_email git commit -am I added logging to my app! git push Access your Github via web browser and verify that you see recent updates and history of changes.","title":"Example implementation of logging"},{"location":"logging/#create-a-docker-image-for-nodejs-application","text":"Use provided Dockerfile to build the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs .","title":"Create a Docker image for node.js application"},{"location":"logging/#integrate-with-the-elastic-stack","text":"The following procedure shows how to send the application logs to the local Elastic stack running in Docker.","title":"Integrate with the Elastic stack"},{"location":"logging/#deploy-a-local-elastic-stack-with-docker-compose","text":"During this lab we will run the Elastic Stack (Elasticsearch, Logstash, Kibana) in the Docker Compose. Configuration for this lab is based on https://github.com/deviantony/docker-elk . In the lab VM the Elastic stack docker compose project was cloned to /root/docker-elk . Briefly review the simple logstash configuration we use for this lab: /root/docker-elk/logstash/pipeline/logstash.conf : input { gelf { port = 5000 } } filter { json { source = message } #we need level field in a numeric format mutate { gsub = [ level , info , 6, level , error , 3 ] } mutate { convert = { level = integer } } } output { elasticsearch { hosts = elasticsearch:9200 } stdout { codec = rubydebug } } The above will reconfigure logstash to use gelf (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using gelf . 1). Start the Elastic stack: cd /root/docker-elk docker-compose up -d Expected output: Creating network docker-elk_elk with driver bridge Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 2). Verify you can access Kibana on http://localhost:5601 .","title":"Deploy a local Elastic stack with Docker Compose"},{"location":"logging/#start-the-nodejs-application-container-and-forward-logs-to-elastic-stack","text":"Start the application container with this command: docker run --name btm-nodejs -d -p 3001:3001 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-nodejs In the lab environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:3001/checkout : for i in {1..10}; do curl http://localhost:3001/checkout; done and check out the Kibana dashboard: Dashboards- BTM Node.js In case of problems with dashboard, you can also import Kibana configuration using provided btm-nodejs-kibana.json : Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json It should be similar to: Stop and remove the node.js Docker container before starting the next exercise. docker stop btm-nodejs docker rm btm-nodejs","title":"Start the node.js application container and forward logs to Elastic stack"},{"location":"monitoring-instrumentation/","text":"What to instrument - the RED method One of the most important decisions to make when setting up web application monitoring is deciding on the type of metrics you need to collect about your app. The metrics you choose simplifies troubleshooting when a problem occurs and also enables you to stay on top of the stability of your services and infrastructure. The RED method follows on the principles outlined in the Four Golden Signals developed by Site Reliability Engineers, which focuses on measuring things that end-users care about when using your web services. With the RED method, three key metrics are instrumented that monitor every microservice in your architecture: (Request) Rate - the number of requests, per second, your services are serving. (Request) Errors - the number of failed requests per second. (Request) Duration - The amount of time each request takes expressed as a time interval. Rate, Errors and Duration attempt to cover the most obvious web service issues. These metrics also capture an error rate that is expressed as a proportion of request rate. Instrument application code with Node.js client library for Prometheus Go to the directory where the server.js file is located and run the following command to install and configure a prom-client - a Node.js client library for Prometheus. cd ~/b2m-nodejs/src npm install --save prom-client this should add the following dependency to the package.json : prom-client : ^11.2.1 Enable default metrics There are some default metrics recommended by Prometheus itself . To collect these, call collectDefaultMetrics NOTE: Some of the metrics, concerning File Descriptors and Memory, are only available on Linux. In addition, some Node-specific metrics are included, such as event loop lag, active handles and Node.js version. See what metrics there are in https://github.com/siimon/prom-client/lib/metrics . collectDefaultMetrics takes 1 options object with 3 entries, a timeout for how often the probe should be fired, an optional prefix for metric names and a registry to which metrics should be registered. By default probes are launched every 10 seconds, but this can be modified like this: const client = require ( prom-client ); const collectDefaultMetrics = client . collectDefaultMetrics ; // Probe every 5th second. collectDefaultMetrics ({ timeout : 5000 }); Lab Instructions: Edit server.js and uncomment the following lines to enable exposure of default set of Node.js metrics on standard Prometheus route /metrics const Prometheus = require ( prom-client ) const metricsInterval = Prometheus . collectDefaultMetrics () and app . get ( /metrics , ( req , res ) = { res . set ( Content-Type , Prometheus . register . contentType ) res . end ( Prometheus . register . metrics ()) }) Test the application locally: cd ~/b2m-nodejs/src npm start server.js Run a couple of transactions by refreshing the URL: http://localhost:3001/checkout Use browser or curl to access http://localhost:3001/metrics to verify exposed metrics. Output should be similar to: # HELP process_cpu_user_seconds_total Total user CPU time spent in seconds. # TYPE process_cpu_user_seconds_total counter process_cpu_user_seconds_total 0.028084 1546452963611 # HELP process_cpu_system_seconds_total Total system CPU time spent in seconds. # TYPE process_cpu_system_seconds_total counter process_cpu_system_seconds_total 0.0038780000000000004 1546452963611 # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds. # TYPE process_cpu_seconds_total counter process_cpu_seconds_total 0.031962 1546452963611 # HELP process_start_time_seconds Start time of the process since unix epoch in seconds. # TYPE process_start_time_seconds gauge process_start_time_seconds 1546452953 # HELP process_resident_memory_bytes Resident memory size in bytes. # TYPE process_resident_memory_bytes gauge process_resident_memory_bytes 29188096 1546452963611 # HELP nodejs_eventloop_lag_seconds Lag of event loop in seconds. # TYPE nodejs_eventloop_lag_seconds gauge nodejs_eventloop_lag_seconds 0.000393303 1546452963612 # HELP nodejs_active_handles_total Number of active handles. # TYPE nodejs_active_handles_total gauge nodejs_active_handles_total 3 1546452963611 # HELP nodejs_active_requests_total Number of active requests. # TYPE nodejs_active_requests_total gauge nodejs_active_requests_total 0 1546452963611 # HELP nodejs_heap_size_total_bytes Process heap size from node.js in bytes. # TYPE nodejs_heap_size_total_bytes gauge nodejs_heap_size_total_bytes 20217856 1546452963611 # HELP nodejs_heap_size_used_bytes Process heap size used from node.js in bytes. # TYPE nodejs_heap_size_used_bytes gauge nodejs_heap_size_used_bytes 8464704 1546452963611 # HELP nodejs_external_memory_bytes Nodejs external memory size in bytes. # TYPE nodejs_external_memory_bytes gauge nodejs_external_memory_bytes 24656 1546452963611 # HELP nodejs_heap_space_size_total_bytes Process heap space size total from node.js in bytes. # TYPE nodejs_heap_space_size_total_bytes gauge nodejs_heap_space_size_total_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_total_bytes{space= new } 8388608 1546452963612 nodejs_heap_space_size_total_bytes{space= old } 8134656 1546452963612 nodejs_heap_space_size_total_bytes{space= code } 1048576 1546452963612 nodejs_heap_space_size_total_bytes{space= map } 1073152 1546452963612 nodejs_heap_space_size_total_bytes{space= large_object } 1572864 1546452963612 # HELP nodejs_heap_space_size_used_bytes Process heap space size used from node.js in bytes. # TYPE nodejs_heap_space_size_used_bytes gauge nodejs_heap_space_size_used_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_used_bytes{space= new } 829768 1546452963612 nodejs_heap_space_size_used_bytes{space= old } 6008448 1546452963612 nodejs_heap_space_size_used_bytes{space= code } 847136 1546452963612 nodejs_heap_space_size_used_bytes{space= map } 533016 1546452963612 nodejs_heap_space_size_used_bytes{space= large_object } 249024 1546452963612 # HELP nodejs_heap_space_size_available_bytes Process heap space size available from node.js in bytes. # TYPE nodejs_heap_space_size_available_bytes gauge nodejs_heap_space_size_available_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_available_bytes{space= new } 3294904 1546452963612 nodejs_heap_space_size_available_bytes{space= old } 1656536 1546452963612 nodejs_heap_space_size_available_bytes{space= code } 0 1546452963612 nodejs_heap_space_size_available_bytes{space= map } 80 1546452963612 nodejs_heap_space_size_available_bytes{space= large_object } 1506500096 1546452963612 # HELP nodejs_version_info Node.js version info. # TYPE nodejs_version_info gauge nodejs_version_info{version= v10.7.0 ,major= 10 ,minor= 7 ,patch= 0 } 1 Stop node.js app ctrl-c . Define custom metric Node.js Prometheus client library allows to define various types of Prometheus metrics like histograms, summaries, gauges and counters. More detailed description of metric types can be found in Prometheus documentation . In this lab we will define two custom metrics: counter checkouts_total which will store a total number of checkout requests histogram http_request_duration_ms which will store percentiles of application requests response time Lab Instructions: Uncomment the rest of commented lines in server.js . checkouts_total Declaration of checkouts_total counter. const checkoutsTotal = new Prometheus . Counter ({ name : checkouts_total , help : Total number of checkouts , labelNames : [ payment_method ] }) This counter will be incremented for every checkout request checkoutsTotal . inc ({ payment_method : paymentMethod }) http_request_duration_ms Declaration of http_request_duration_ms histogram: const httpRequestDurationMicroseconds = new Prometheus . Histogram ({ name : http_request_duration_ms , help : Duration of HTTP requests in ms , labelNames : [ method , route , code ], buckets : [ 0.10 , 5 , 15 , 50 , 100 , 200 , 300 , 400 , 500 ] // buckets for response time from 0.1ms to 500ms }) The current time is recorded before each request: app . use (( req , res , next ) = { res . locals . startEpoch = Date . now () next () }) We record the current time also after each request and update our http_request_duration_ms histogram accordingly: app . use (( req , res , next ) = { const responseTimeInMs = Date . now () - res . locals . startEpoch httpRequestDurationMicroseconds . labels ( req . method , req . route . path , res . statusCode ) . observe ( responseTimeInMs ) next () }) After you complete code changes, start the local application instance: cd ~/b2m-nodejs/src npm start server.js Run a couple of transactions by refreshing the URL: http://localhost:3001/checkout Use browser to access http://localhost:3001/metrics to verify exposed metrics. Output should be similar to: (...) # HELP checkouts_total Total number of checkouts # TYPE checkouts_total counter checkouts_total{payment_method= paypal } 7 checkouts_total{payment_method= stripe } 5 # HELP http_request_duration_ms Duration of HTTP requests in ms # TYPE http_request_duration_ms histogram http_request_duration_ms_bucket{le= 0.1 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 5 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 15 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 50 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 100 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 200 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 300 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 400 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 500 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= +Inf ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_sum{method= GET ,route= / ,code= 304 } 415 http_request_duration_ms_count{method= GET ,route= / ,code= 304 } 3 http_request_duration_ms_bucket{le= 0.1 ,code= 500 ,route= /bad ,method= GET } 0 http_request_duration_ms_bucket{le= 5 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 15 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 50 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 100 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 200 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 300 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 400 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 500 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= +Inf ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_sum{method= GET ,route= /bad ,code= 500 } 1 http_request_duration_ms_count{method= GET ,route= /bad ,code= 500 } 1 http_request_duration_ms_bucket{le= 0.1 ,code= 304 ,route= /checkout ,method= GET } 8 http_request_duration_ms_bucket{le= 5 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 15 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 50 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 100 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 200 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 300 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 400 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 500 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= +Inf ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_sum{method= GET ,route= /checkout ,code= 304 } 4 http_request_duration_ms_count{method= GET ,route= /checkout ,code= 304 } 12 Besides the default set of metrics related to resource utilization by the application process, we can see the additional metrics: checkouts_total by payment_method http_request_duration_ms_bucket by percentile buckets and by route Stop the node.js application with ctrl-c . Commit and push changes to your GitHub repository and create a pull request Commit your changes to your GiHub repository: cd ~/b2m-nodejs git commit -am I added monitoring instrumentation to my app! git push Logon to your GitHub account via web browser and go to the b2m-nodejs repository. Create a pull request to submit your changes to the source Github repository. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Click the New pull request button, quickly review your changes and then click Create pull request button. Write a title and description of your changes and then click Create pull request . Build and test the Docker image Use provided Dockerfile to rebuild the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs . Make sure the application is not running and you removed the previous application docker container: docker ps|grep btm-nodejs Start the updated docker container: docker run --name btm-nodejs -d -p 3001:3001 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-nodejs Verify URLs: http://localhost:3001/checkout and http://localhost:3001/metrics to make sure everything works correctly.","title":"3. Node.js monitoring instrumentation"},{"location":"monitoring-instrumentation/#what-to-instrument-the-red-method","text":"One of the most important decisions to make when setting up web application monitoring is deciding on the type of metrics you need to collect about your app. The metrics you choose simplifies troubleshooting when a problem occurs and also enables you to stay on top of the stability of your services and infrastructure. The RED method follows on the principles outlined in the Four Golden Signals developed by Site Reliability Engineers, which focuses on measuring things that end-users care about when using your web services. With the RED method, three key metrics are instrumented that monitor every microservice in your architecture: (Request) Rate - the number of requests, per second, your services are serving. (Request) Errors - the number of failed requests per second. (Request) Duration - The amount of time each request takes expressed as a time interval. Rate, Errors and Duration attempt to cover the most obvious web service issues. These metrics also capture an error rate that is expressed as a proportion of request rate.","title":"What to instrument - the RED method"},{"location":"monitoring-instrumentation/#instrument-application-code-with-nodejs-client-library-for-prometheus","text":"Go to the directory where the server.js file is located and run the following command to install and configure a prom-client - a Node.js client library for Prometheus. cd ~/b2m-nodejs/src npm install --save prom-client this should add the following dependency to the package.json : prom-client : ^11.2.1","title":"Instrument application code with Node.js client library for Prometheus"},{"location":"monitoring-instrumentation/#enable-default-metrics","text":"There are some default metrics recommended by Prometheus itself . To collect these, call collectDefaultMetrics NOTE: Some of the metrics, concerning File Descriptors and Memory, are only available on Linux. In addition, some Node-specific metrics are included, such as event loop lag, active handles and Node.js version. See what metrics there are in https://github.com/siimon/prom-client/lib/metrics . collectDefaultMetrics takes 1 options object with 3 entries, a timeout for how often the probe should be fired, an optional prefix for metric names and a registry to which metrics should be registered. By default probes are launched every 10 seconds, but this can be modified like this: const client = require ( prom-client ); const collectDefaultMetrics = client . collectDefaultMetrics ; // Probe every 5th second. collectDefaultMetrics ({ timeout : 5000 }); Lab Instructions: Edit server.js and uncomment the following lines to enable exposure of default set of Node.js metrics on standard Prometheus route /metrics const Prometheus = require ( prom-client ) const metricsInterval = Prometheus . collectDefaultMetrics () and app . get ( /metrics , ( req , res ) = { res . set ( Content-Type , Prometheus . register . contentType ) res . end ( Prometheus . register . metrics ()) }) Test the application locally: cd ~/b2m-nodejs/src npm start server.js Run a couple of transactions by refreshing the URL: http://localhost:3001/checkout Use browser or curl to access http://localhost:3001/metrics to verify exposed metrics. Output should be similar to: # HELP process_cpu_user_seconds_total Total user CPU time spent in seconds. # TYPE process_cpu_user_seconds_total counter process_cpu_user_seconds_total 0.028084 1546452963611 # HELP process_cpu_system_seconds_total Total system CPU time spent in seconds. # TYPE process_cpu_system_seconds_total counter process_cpu_system_seconds_total 0.0038780000000000004 1546452963611 # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds. # TYPE process_cpu_seconds_total counter process_cpu_seconds_total 0.031962 1546452963611 # HELP process_start_time_seconds Start time of the process since unix epoch in seconds. # TYPE process_start_time_seconds gauge process_start_time_seconds 1546452953 # HELP process_resident_memory_bytes Resident memory size in bytes. # TYPE process_resident_memory_bytes gauge process_resident_memory_bytes 29188096 1546452963611 # HELP nodejs_eventloop_lag_seconds Lag of event loop in seconds. # TYPE nodejs_eventloop_lag_seconds gauge nodejs_eventloop_lag_seconds 0.000393303 1546452963612 # HELP nodejs_active_handles_total Number of active handles. # TYPE nodejs_active_handles_total gauge nodejs_active_handles_total 3 1546452963611 # HELP nodejs_active_requests_total Number of active requests. # TYPE nodejs_active_requests_total gauge nodejs_active_requests_total 0 1546452963611 # HELP nodejs_heap_size_total_bytes Process heap size from node.js in bytes. # TYPE nodejs_heap_size_total_bytes gauge nodejs_heap_size_total_bytes 20217856 1546452963611 # HELP nodejs_heap_size_used_bytes Process heap size used from node.js in bytes. # TYPE nodejs_heap_size_used_bytes gauge nodejs_heap_size_used_bytes 8464704 1546452963611 # HELP nodejs_external_memory_bytes Nodejs external memory size in bytes. # TYPE nodejs_external_memory_bytes gauge nodejs_external_memory_bytes 24656 1546452963611 # HELP nodejs_heap_space_size_total_bytes Process heap space size total from node.js in bytes. # TYPE nodejs_heap_space_size_total_bytes gauge nodejs_heap_space_size_total_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_total_bytes{space= new } 8388608 1546452963612 nodejs_heap_space_size_total_bytes{space= old } 8134656 1546452963612 nodejs_heap_space_size_total_bytes{space= code } 1048576 1546452963612 nodejs_heap_space_size_total_bytes{space= map } 1073152 1546452963612 nodejs_heap_space_size_total_bytes{space= large_object } 1572864 1546452963612 # HELP nodejs_heap_space_size_used_bytes Process heap space size used from node.js in bytes. # TYPE nodejs_heap_space_size_used_bytes gauge nodejs_heap_space_size_used_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_used_bytes{space= new } 829768 1546452963612 nodejs_heap_space_size_used_bytes{space= old } 6008448 1546452963612 nodejs_heap_space_size_used_bytes{space= code } 847136 1546452963612 nodejs_heap_space_size_used_bytes{space= map } 533016 1546452963612 nodejs_heap_space_size_used_bytes{space= large_object } 249024 1546452963612 # HELP nodejs_heap_space_size_available_bytes Process heap space size available from node.js in bytes. # TYPE nodejs_heap_space_size_available_bytes gauge nodejs_heap_space_size_available_bytes{space= read_only } 0 1546452963612 nodejs_heap_space_size_available_bytes{space= new } 3294904 1546452963612 nodejs_heap_space_size_available_bytes{space= old } 1656536 1546452963612 nodejs_heap_space_size_available_bytes{space= code } 0 1546452963612 nodejs_heap_space_size_available_bytes{space= map } 80 1546452963612 nodejs_heap_space_size_available_bytes{space= large_object } 1506500096 1546452963612 # HELP nodejs_version_info Node.js version info. # TYPE nodejs_version_info gauge nodejs_version_info{version= v10.7.0 ,major= 10 ,minor= 7 ,patch= 0 } 1 Stop node.js app ctrl-c .","title":"Enable default metrics"},{"location":"monitoring-instrumentation/#define-custom-metric","text":"Node.js Prometheus client library allows to define various types of Prometheus metrics like histograms, summaries, gauges and counters. More detailed description of metric types can be found in Prometheus documentation . In this lab we will define two custom metrics: counter checkouts_total which will store a total number of checkout requests histogram http_request_duration_ms which will store percentiles of application requests response time Lab Instructions: Uncomment the rest of commented lines in server.js .","title":"Define custom metric"},{"location":"monitoring-instrumentation/#checkouts_total","text":"Declaration of checkouts_total counter. const checkoutsTotal = new Prometheus . Counter ({ name : checkouts_total , help : Total number of checkouts , labelNames : [ payment_method ] }) This counter will be incremented for every checkout request checkoutsTotal . inc ({ payment_method : paymentMethod })","title":"checkouts_total"},{"location":"monitoring-instrumentation/#http95request95duration95ms","text":"Declaration of http_request_duration_ms histogram: const httpRequestDurationMicroseconds = new Prometheus . Histogram ({ name : http_request_duration_ms , help : Duration of HTTP requests in ms , labelNames : [ method , route , code ], buckets : [ 0.10 , 5 , 15 , 50 , 100 , 200 , 300 , 400 , 500 ] // buckets for response time from 0.1ms to 500ms }) The current time is recorded before each request: app . use (( req , res , next ) = { res . locals . startEpoch = Date . now () next () }) We record the current time also after each request and update our http_request_duration_ms histogram accordingly: app . use (( req , res , next ) = { const responseTimeInMs = Date . now () - res . locals . startEpoch httpRequestDurationMicroseconds . labels ( req . method , req . route . path , res . statusCode ) . observe ( responseTimeInMs ) next () }) After you complete code changes, start the local application instance: cd ~/b2m-nodejs/src npm start server.js Run a couple of transactions by refreshing the URL: http://localhost:3001/checkout Use browser to access http://localhost:3001/metrics to verify exposed metrics. Output should be similar to: (...) # HELP checkouts_total Total number of checkouts # TYPE checkouts_total counter checkouts_total{payment_method= paypal } 7 checkouts_total{payment_method= stripe } 5 # HELP http_request_duration_ms Duration of HTTP requests in ms # TYPE http_request_duration_ms histogram http_request_duration_ms_bucket{le= 0.1 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 5 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 15 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 50 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 100 ,code= 304 ,route= / ,method= GET } 0 http_request_duration_ms_bucket{le= 200 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 300 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 400 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= 500 ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_bucket{le= +Inf ,code= 304 ,route= / ,method= GET } 3 http_request_duration_ms_sum{method= GET ,route= / ,code= 304 } 415 http_request_duration_ms_count{method= GET ,route= / ,code= 304 } 3 http_request_duration_ms_bucket{le= 0.1 ,code= 500 ,route= /bad ,method= GET } 0 http_request_duration_ms_bucket{le= 5 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 15 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 50 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 100 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 200 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 300 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 400 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= 500 ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_bucket{le= +Inf ,code= 500 ,route= /bad ,method= GET } 1 http_request_duration_ms_sum{method= GET ,route= /bad ,code= 500 } 1 http_request_duration_ms_count{method= GET ,route= /bad ,code= 500 } 1 http_request_duration_ms_bucket{le= 0.1 ,code= 304 ,route= /checkout ,method= GET } 8 http_request_duration_ms_bucket{le= 5 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 15 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 50 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 100 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 200 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 300 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 400 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= 500 ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_bucket{le= +Inf ,code= 304 ,route= /checkout ,method= GET } 12 http_request_duration_ms_sum{method= GET ,route= /checkout ,code= 304 } 4 http_request_duration_ms_count{method= GET ,route= /checkout ,code= 304 } 12 Besides the default set of metrics related to resource utilization by the application process, we can see the additional metrics: checkouts_total by payment_method http_request_duration_ms_bucket by percentile buckets and by route Stop the node.js application with ctrl-c .","title":"http_request_duration_ms"},{"location":"monitoring-instrumentation/#commit-and-push-changes-to-your-github-repository-and-create-a-pull-request","text":"Commit your changes to your GiHub repository: cd ~/b2m-nodejs git commit -am I added monitoring instrumentation to my app! git push Logon to your GitHub account via web browser and go to the b2m-nodejs repository. Create a pull request to submit your changes to the source Github repository. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Click the New pull request button, quickly review your changes and then click Create pull request button. Write a title and description of your changes and then click Create pull request .","title":"Commit and push changes to your GitHub repository and create a pull request"},{"location":"monitoring-instrumentation/#build-and-test-the-docker-image","text":"Use provided Dockerfile to rebuild the application container: cd ~/b2m-nodejs/src docker build -t b2m-nodejs . Make sure the application is not running and you removed the previous application docker container: docker ps|grep btm-nodejs Start the updated docker container: docker run --name btm-nodejs -d -p 3001:3001 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-nodejs Verify URLs: http://localhost:3001/checkout and http://localhost:3001/metrics to make sure everything works correctly.","title":"Build and test the Docker image"}]}